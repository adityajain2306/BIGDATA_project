{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb44fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5eaf79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Bank Customer Segmentation\").enableHiveSupport().getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa8d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file_path is Bank_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, when, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType, IntegerType\n",
    "data_path = \"Bank_dataset.csv\"\n",
    "\n",
    "# Print the file_path\n",
    "print(\"The file_path is\", data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c052c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|TransactionID|CustomerID|CustomerDOB|CustGender|CustLocation|CustAccountBalance|TransactionDate|TransactionTime|TransactionAmount (INR)|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|           T1|  C5841053| 10-01-1994|         F|  JAMSHEDPUR|          17819.05|     02-08-2016|         143207|                   25.0|\n",
      "|           T2|  C2142763| 04-04-1957|         M|     JHAJJAR|           2270.69|     02-08-2016|         141858|                27999.0|\n",
      "|           T3|  C4417068| 26-11-1996|         F|      MUMBAI|          17874.44|     02-08-2016|         142712|                  459.0|\n",
      "|           T4|  C5342380| 14-09-1973|         F|      MUMBAI|         866503.21|     02-08-2016|         142714|                 2060.0|\n",
      "|           T5|  C9031234| 24-03-1988|         F| NAVI MUMBAI|           6714.43|     02-08-2016|         181156|                 1762.5|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TransactionID: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- CustomerDOB: string (nullable = true)\n",
      " |-- CustGender: string (nullable = true)\n",
      " |-- CustLocation: string (nullable = true)\n",
      " |-- CustAccountBalance: double (nullable = true)\n",
      " |-- TransactionDate: string (nullable = true)\n",
      " |-- TransactionTime: integer (nullable = true)\n",
      " |-- TransactionAmount (INR): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Inspect Data\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "588c3b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+-----------+----------+--------------------+------------------+---------------+------------------+-----------------------+\n",
      "|summary|TransactionID|CustomerID|CustomerDOB|CustGender|        CustLocation|CustAccountBalance|TransactionDate|   TransactionTime|TransactionAmount (INR)|\n",
      "+-------+-------------+----------+-----------+----------+--------------------+------------------+---------------+------------------+-----------------------+\n",
      "|  count|      1048567|   1048567|    1048567|   1047467|             1048416|           1046198|        1048567|           1048567|                1048567|\n",
      "|   mean|         null|      null|       null|      null|            400012.0|115403.54005622343|           null|157087.52939297154|     1574.3350034571733|\n",
      "| stddev|         null|      null|       null|      null|                 0.0|  846485.380600677|           null| 51261.85402233114|      6574.742978453954|\n",
      "|    min|           T1|  C1010011| 01-01-1930|         F|(154) BHASKOLA FA...|               0.0|     01-08-2016|                 0|                    0.0|\n",
      "|    max|      T999999|  C9099956|        nan|         T|           ZUNHEBOTO|     1.150354951E8|     31-08-2016|            235959|             1560034.99|\n",
      "+-------+-------------+----------+-----------+----------+--------------------+------------------+---------------+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary Statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d670ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|TransactionID|CustomerID|CustomerDOB|CustGender|CustLocation|CustAccountBalance|TransactionDate|TransactionTime|TransactionAmount (INR)|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|            0|         0|          0|      1100|         151|              2369|              0|              0|                      0|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for Null Values\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "null_counts = df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e46c973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|TransactionID|count|\n",
      "+-------------+-----+\n",
      "|         T352|    1|\n",
      "|         T590|    1|\n",
      "|         T855|    1|\n",
      "|         T929|    1|\n",
      "|         T947|    1|\n",
      "|        T1118|    1|\n",
      "|        T1401|    1|\n",
      "|        T1508|    1|\n",
      "|        T1767|    1|\n",
      "|        T1872|    1|\n",
      "|        T2345|    1|\n",
      "|        T2463|    1|\n",
      "|        T2837|    1|\n",
      "|        T2947|    1|\n",
      "|        T3091|    1|\n",
      "|        T3230|    1|\n",
      "|        T3271|    1|\n",
      "|        T3337|    1|\n",
      "|        T3396|    1|\n",
      "|        T4155|    1|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-----+\n",
      "|CustomerID|count|\n",
      "+----------+-----+\n",
      "|  C8732711|    1|\n",
      "|  C6421261|    2|\n",
      "|  C2939112|    2|\n",
      "|  C8741166|    1|\n",
      "|  C4440931|    1|\n",
      "|  C8116854|    1|\n",
      "|  C4029960|    1|\n",
      "|  C7817677|    4|\n",
      "|  C4240562|    1|\n",
      "|  C2221420|    1|\n",
      "|  C8524541|    1|\n",
      "|  C5638051|    3|\n",
      "|  C2331867|    1|\n",
      "|  C5534211|    1|\n",
      "|  C4940219|    1|\n",
      "|  C2230276|    1|\n",
      "|  C4341556|    1|\n",
      "|  C7283217|    1|\n",
      "|  C3825339|    1|\n",
      "|  C4138928|    1|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|CustomerDOB|count|\n",
      "+-----------+-----+\n",
      "| 11-04-1975|   24|\n",
      "| 24-08-1990|  156|\n",
      "| 19-02-1983|  263|\n",
      "| 24-02-1972|   45|\n",
      "| 11-06-1995|   85|\n",
      "| 31-08-1993|  142|\n",
      "| 14-05-1985|   96|\n",
      "| 09-09-1970|   52|\n",
      "| 21-04-1991|  109|\n",
      "| 19-02-1952|    4|\n",
      "| 09-05-1994|  127|\n",
      "| 30-08-1969|   61|\n",
      "| 08-10-1980|  136|\n",
      "| 30-05-1954|    5|\n",
      "| 27-10-1973|  103|\n",
      "| 14-10-1978|   73|\n",
      "| 14-05-1978|   27|\n",
      "| 29-03-1993|   90|\n",
      "| 11-05-1992|  139|\n",
      "| 20-06-1982|  140|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+------+\n",
      "|CustGender| count|\n",
      "+----------+------+\n",
      "|         F|281936|\n",
      "|      null|  1100|\n",
      "|         T|     1|\n",
      "|         M|765530|\n",
      "+----------+------+\n",
      "\n",
      "+--------------------+-----+\n",
      "|        CustLocation|count|\n",
      "+--------------------+-----+\n",
      "|             GWALIOR|  948|\n",
      "|      UTTAR DINAJPUR|   62|\n",
      "|             SANGRUR|  353|\n",
      "|  ACADEMY SAMASTIPUR|    5|\n",
      "|NO 3 NR LALLAN HO...|   57|\n",
      "|GATE DARIYAPUR AH...|    7|\n",
      "|ELPHINSTONE W MUMBAI|   12|\n",
      "|             SHIMOGA|  114|\n",
      "|          FLOOR PUNE|    2|\n",
      "|           TIRUVALLA|    8|\n",
      "|        SARAN CHAPRA|    3|\n",
      "|             MOHANIA|    2|\n",
      "|              TEEKOY|   10|\n",
      "|CUMBALLA HILL MUMBAI|    2|\n",
      "|            KANUBARI|   24|\n",
      "|          NALAPPURAM|    5|\n",
      "|B/H ATM UNION BAN...|    5|\n",
      "|  TIRUPATI NGR THANE|    2|\n",
      "|            K K DIST|   10|\n",
      "|              SUBBAL|    3|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|TransactionDate|count|\n",
      "+---------------+-----+\n",
      "|     05-08-2016|21112|\n",
      "|     10-08-2016|21649|\n",
      "|     22-09-2016| 6971|\n",
      "|     02-09-2016|22839|\n",
      "|     25-09-2016| 8164|\n",
      "|     09-08-2016|21823|\n",
      "|     29-08-2016|16882|\n",
      "|     01-08-2016|20438|\n",
      "|     22-08-2016|18558|\n",
      "|     07-09-2016|21161|\n",
      "|     23-09-2016| 3485|\n",
      "|     07-08-2016|27261|\n",
      "|     26-08-2016|17742|\n",
      "|     14-08-2016|25596|\n",
      "|     11-09-2016|25454|\n",
      "|     12-09-2016|20753|\n",
      "|     24-08-2016|18026|\n",
      "|     30-08-2016|17430|\n",
      "|     14-09-2016|19969|\n",
      "|     15-09-2016|19754|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Value Counts for Categorical Features\n",
    "categorical_columns = [col_name for col_name, dtype in df.dtypes if dtype == 'string']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74ce51d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|CustAccountBalance|\n",
      "+-------+------------------+\n",
      "|  count|           1046198|\n",
      "|   mean|115403.54005622343|\n",
      "| stddev|  846485.380600677|\n",
      "|    min|               0.0|\n",
      "|    25%|           4721.76|\n",
      "|    50%|          16794.15|\n",
      "|    75%|          57646.03|\n",
      "|    max|     1.150354951E8|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|   TransactionTime|\n",
      "+-------+------------------+\n",
      "|  count|           1048567|\n",
      "|   mean|157087.52939297154|\n",
      "| stddev| 51261.85402233114|\n",
      "|    min|                 0|\n",
      "|    25%|            124033|\n",
      "|    50%|            164221|\n",
      "|    75%|            200012|\n",
      "|    max|            235959|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+-----------------------+\n",
      "|summary|TransactionAmount (INR)|\n",
      "+-------+-----------------------+\n",
      "|  count|                1048567|\n",
      "|   mean|     1574.3350034571733|\n",
      "| stddev|      6574.742978453954|\n",
      "|    min|                    0.0|\n",
      "|    25%|                  160.2|\n",
      "|    50%|                 459.01|\n",
      "|    75%|                 1200.0|\n",
      "|    max|             1560034.99|\n",
      "+-------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Correlation Analysis (for numerical features)\n",
    "numeric_columns = [col_name for col_name, dtype in df.dtypes if dtype in ['int', 'double']]\n",
    "\n",
    "for column in numeric_columns:\n",
    "    df.select(column).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb89c892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|TransactionID|CustomerID|CustomerDOB|CustGender|CustLocation|CustAccountBalance|TransactionDate|TransactionTime|TransactionAmount (INR)|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|            0|         0|          0|         0|           0|                 0|              0|              0|                      0|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with null values in specific columns\n",
    "df = df.na.drop(subset=[\"CustGender\", \"CustLocation\", \"CustAccountBalance\"])\n",
    "\n",
    "# Verify that null values are dropped\n",
    "null_counts_after_drop = df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns])\n",
    "null_counts_after_drop.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e78f9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=numeric_columns, outputCol=vector_col)\n",
    "vector_df = assembler.transform(df).select(vector_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bb30cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.00410334  0.06264685]\n",
      " [-0.00410334  1.          0.00787137]\n",
      " [ 0.06264685  0.00787137  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "matrix = Correlation.corr(vector_df, vector_col)\n",
    "correlation_matrix = matrix.collect()[0][0]\n",
    "print(correlation_matrix.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86b0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bda22b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
